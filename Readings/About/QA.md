Vinny Monteiro, a business transformation manager at JP Morgan Chase
on delivering exceptional products

### The Quality Equation

a powerful concept that helps us balance the value we deliver to users with the cost of development

The key lies in maximizing the value for the cost of the product or feature. 
If you focus on delivering features and functionalities that truly matter to users, you can maximize value while keeping development costs under control. 

The quality equation highlights the cost effectiveness of proactive quality measures. 

The quality equation reminds us that there is a tipping point. While striving for excellence is important, obsessing over every tiny detail can be counterproductive. 

Imagine going on a long hike. You need to balance between traveling light versus bringing everything you need because you would get bogged down by carrying an excessive amount of weight. 

You should focus on features and functionalities that deliver real value to users, but avoid getting hung up on excessive perfectionism or features that add little value. 


## ask deepseek about judgemental call is DoD
should "Definition of Done" contain some subjective judgmental calls

Thought for 18 seconds

Okay, so the user is asking whether the "Definition of Done" (DoD) should include some subjective judgment calls. Let me start by recalling what the DoD is. From what I know, in Agile methodologies like Scrum, the DoD is a set of criteria that a product increment must meet to be considered complete. It's supposed to be a clear and shared understanding among the team.

Now, the question is about whether there's room for subjectivity in that. My first thought is that ideally, the DoD should be as objective as possible to avoid ambiguity. If it's subjective, different team members might interpret it differently, leading to inconsistencies. For example, if a DoD item is "Code is well-tested," that's pretty vague. What does "well-tested" mean? One developer might think unit tests are enough, another might think integration tests are necessary. That could cause problems in quality or delays.

But maybe there are areas where some subjectivity is unavoidable. Let's say a team includes a requirement like "User interface is intuitive." Intuitiveness is subjective by nature. It's based on user perception, which can vary. So in such cases, how does the team handle that? They might need to rely on user testing or expert reviews, which introduces some judgment but based on feedback rather than personal opinion.

Another angle is the maturity of the team. Newer teams might have a less refined DoD that includes more subjective measures because they're still learning. Over time, as they gain experience, they can replace those subjective items with more objective ones. For instance, instead of saying "Code is clean," they might adopt specific coding standards or use static analysis tools to measure code quality.

Also, different contexts might require different approaches. In creative fields like design or content creation, some subjectivity is inherent. A DoD for a design task might include "Approved by the client," which is a subjective judgment but necessary. However, even there, teams can aim to make the approval criteria as clear as possible to reduce ambiguity.

I should also consider the balance between flexibility and rigor. Too much subjectivity might lead to scope creep or incomplete work, but too much rigidity could stifle creativity or adaptability. The key is finding a middle ground where the DoD is mostly objective but allows for expert judgment in areas where it's needed.

Examples might help. If the DoD includes "All tests pass," that's objective. If it includes "Code reviewed by two peers," that's also objective. But if it includes "User story meets acceptance criteria," and the acceptance criteria are vague, then it's subjective. So the solution is to make sure acceptance criteria are well-defined.

In conclusion, while the DoD should primarily be objective to ensure consistency and clarity, there might be scenarios where some subjectivity is necessary. However, teams should strive to minimize it by defining clear, measurable criteria wherever possible and using agreed-upon processes (like user testing or expert review) to handle areas where subjectivity can't be eliminated entirely.

The **Definition of Done (DoD)** should ideally minimize subjectivity to ensure consistency, transparency, and shared understanding within a team. However, in practice, some judgment may be unavoidable depending on context. Here's a breakdown:

### **1. Objective Criteria Are Preferred**

- **Clarity and Consistency**: Objective criteria (e.g., "All unit tests pass," "Code reviewed by two peers") reduce ambiguity and ensure everyone agrees on what "done" means. This prevents disagreements and uneven quality.
    
- **Measurability**: Metrics like test coverage percentages, compliance with coding standards, or automated checks leave little room for interpretation.
    

### **2. When Subjectivity _Might_ Creep In**

- **Design/Creative Work**: Criteria like "UI aligns with brand guidelines" or "user-friendly" may require expert judgment. Teams can mitigate this by referencing style guides, user testing, or stakeholder sign-offs.
    
- **Complex or Novel Problems**: Early-stage projects might lack clear benchmarks, requiring experienced team members to assess readiness (e.g., "solution is scalable enough for MVP").
    
- **Cultural Norms**: Teams may implicitly rely on shared standards (e.g., "clean code"), but these should evolve into explicit, measurable rules over time.
    

### **3. Mitigating Subjectivity**

- **Refine the DoD Iteratively**: Replace vague terms (e.g., "well-tested") with specifics (e.g., "80% test coverage, integration tests included").
    
- **Use Guardrails for Judgment**: If subjective criteria are unavoidable, define _who_ decides (e.g., "approved by UX lead") or _how_ (e.g., "validated via user testing with 5+ participants").
    
- **Balance Flexibility and Rigor**: In creative fields, pair subjective goals (e.g., "client approves") with objective checks (e.g., "meets accessibility standards").
    

### **4. Risks of Excessive Subjectivity**

- Inconsistent quality, scope creep, or delays if team members interpret criteria differently.
    
- Erosion of trust if stakeholders perceive the DoD as arbitrary.
    

### **Conclusion**

Aim for **objectivity first**, but acknowledge that **some judgment may be necessary** in nuanced scenarios. Teams should document and refine the DoD collaboratively to minimize ambiguity while allowing flexibility where expertise is required. Over time, replace subjective elements with measurable standards as the team matures.


## asked copilot
The "Definition of Done" (DoD) is a critical concept in Agile development, ensuring that all the criteria are met for a feature or task before it can be considered complete. Ideally, the DoD should be as objective and quantifiable as possible to avoid ambiguity. However, there might be some scenarios where subjective judgmental calls are necessary. Here's a breakdown of when it might be appropriate:

### When to Include Subjective Judgmental Calls:
- **Quality Assurance**: Certain quality aspects like user experience or design aesthetics may require a subjective assessment to ensure they meet the desired standards.
- **Stakeholder Feedback**: When gathering feedback from stakeholders or end-users, some subjective judgment may be necessary to interpret and act on their input.
- **Innovative Features**: For innovative or creative features, it might be challenging to define objective criteria. In such cases, subjective judgment can help gauge whether the feature meets its intended goals.

### When to Avoid Subjective Judgmental Calls:
- **Technical Completion**: Tasks like code implementation, unit tests, or documentation should have clear, objective criteria to avoid discrepancies.
- **Compliance and Regulations**: Any criteria related to legal, regulatory, or compliance requirements must be strictly objective to ensure adherence.

Ultimately, while the core of the DoD should be objective, including some subjective elements can be valuable for ensuring quality and user satisfaction. It’s about finding the right balance to meet your project's specific needs.

Do you have any particular examples or scenarios in mind where you're considering incorporating subjective judgment into your DoD? It might help to discuss those specific cases!





## why scrum define dod

<thought>
Okay, the user is asking why Scrum defines the Definition of Done. Let me start by recalling what the Definition of Done (DoD) is in Scrum. From previous conversations, I know that the DoD is a set of criteria that a product increment must meet to be considered complete. But why does Scrum emphasize this?

First, I should think about the core principles of Scrum. Scrum is all about iterative development, transparency, inspection, and adaptation. The DoD plays a role in ensuring that each increment is potentially shippable. Without a clear DoD, teams might have different understandings of what "done" means, leading to inconsistencies and quality issues.

The user might be coming from a scenario where their team has faced issues with incomplete work or miscommunication. They might want to understand the underlying reasons Scrum insists on the DoD to better implement it in their own processes.

I should break down the main reasons Scrum defines the DoD. Transparency is a big one. The DoD makes the standards explicit so everyone knows what's expected. Quality assurance is another; by having a checklist, teams ensure they don't skip crucial steps like testing or documentation.

Also, the DoD helps in managing stakeholder expectations. If everyone agrees on what done means, there's less room for disagreement later. It also supports continuous improvement because the team can refine the DoD as they learn more about their process.

I need to make sure to mention how the DoD contributes to the Scrum pillars: transparency, inspection, and adaptation. Maybe give examples of what a DoD might include, like code reviews, testing, documentation. Also, highlight that the DoD is agreed upon by the whole team, which fosters collective ownership.

I should also address potential misconceptions. Some might think the DoD is too rigid, but in reality, it's meant to evolve. The team updates it as they improve. Also, it's not just for developers; stakeholders benefit from knowing the work meets certain standards.

Finally, summarize the key points to reinforce why Scrum includes the DoD. It's all about ensuring quality, alignment, and continuous improvement. Maybe end with a note on how teams can start defining their DoD if they haven't already.
</thought>


Scrum defines a **Definition of Done (DoD)** to ensure clarity, consistency, and quality in delivering increments of work. The DoD acts as a shared agreement within the Scrum Team (and with stakeholders) about what it means for a product backlog item to be truly "done." Here’s why Scrum emphasizes this concept:

---

### **1. Ensures Transparency**  
   - The DoD eliminates ambiguity about what "done" means, ensuring everyone (developers, Product Owner, stakeholders) has the same expectations.  
   - Without a DoD, teams risk delivering incomplete or inconsistent work, leading to confusion and technical debt.

---

### **2. Guarantees Quality**  
   - The DoD enforces a baseline of quality by defining non-negotiable criteria (e.g., "code reviewed," "tested," "documented").  
   - It prevents teams from cutting corners (e.g., skipping testing or documentation) to meet deadlines.

---

### **3. Supports Incremental Delivery**  
   - Scrum requires a **potentially shippable increment** at the end of each Sprint. The DoD ensures every increment meets the team’s quality standards.  
   - Without a DoD, increments might lack essential elements (e.g., missing user acceptance tests), making them unusable.

---

### **4. Facilitates Continuous Improvement**  
   - The DoD is not static. Teams regularly refine it during Sprint Retrospectives to address gaps (e.g., adding security scans or performance benchmarks).  
   - This fosters a culture of learning and adaptation.

---

### **5. Reduces Risk**  
   - By explicitly defining completion criteria, the DoD minimizes the risk of hidden work or surprises (e.g., untested code or unresolved bugs).  
   - Stakeholders gain confidence that deliverables meet agreed standards.

---

### **6. Aligns the Team**  
   - The DoD creates a shared understanding of responsibilities. For example, it clarifies whether tasks like UX reviews or DevOps deployments are part of the team’s workflow.  
   - It prevents the "90% done" fallacy (where work is declared "done" prematurely).

---

### **What’s Typically Included in a DoD?**  
While the specifics vary by team, a DoD often includes criteria like:  
- Code reviewed and merged.  
- Automated tests passing.  
- Documentation updated.  
- Performance benchmarks met.  
- Stakeholder/UAT approval.  
- Deployed to a staging environment.  

---

### **Example Scenario**  
Imagine a team *without* a DoD:  
- A developer marks a feature "done" after writing code but skips testing.  
- The Product Owner expects the feature to be fully tested and deployable.  
- Misalignment leads to delays and frustration.  

With a DoD:  
- The team agrees that "done" means code is reviewed, tested, documented, and deployed to staging.  
- Everyone knows the standards upfront, reducing conflicts.

---

### **Key Takeaway**  
The **Definition of Done** is foundational to Scrum because it:  
1. **Standardizes quality** across the product.  
2. **Builds trust** with stakeholders.  
3. **Enables predictability** in delivery.  
4. **Empowers teams** to own their process.  

If a team skips the DoD, they risk technical debt, miscommunication, and inconsistent outcomes. Scrum formalizes the DoD to ensure work is truly *done*—not just *developed*.



## dod is on incremental
In Scrum, the **Definition of Done (DoD)** is **defined at the Increment level**, not at the individual Product Backlog Item (PBI) level. However, PBIs (e.g., user stories, features) often have their own **acceptance criteria**, which complement the DoD. Here’s how they work together:

---

### **1. Definition of Done (DoD) Applies to the Increment**
   - The DoD is a **team-wide agreement** that defines the quality standards and completeness criteria for the **entire Increment** (the sum of all PBIs completed in a Sprint).  
   - **Example DoD Criteria**:  
     - Code reviewed and merged.  
     - All automated tests pass.  
     - Documentation updated.  
     - Deployed to staging.  
     - Performance benchmarks met.  
   - **Purpose**: Ensures every Increment is **potentially shippable** and meets a consistent quality baseline.

---

### **2. PBIs Have Acceptance Criteria**
   - **Acceptance Criteria** are specific, granular conditions that a **single PBI** must satisfy to be considered "done" from a functional perspective.  
   - **Example Acceptance Criteria for a PBI**:  
     - "User can reset their password via email."  
     - "Search results load within 2 seconds."  
   - **Purpose**: Defines **what** the PBI must achieve (scope and functionality).

---

### **Key Differences**
| **Definition of Done** | **Acceptance Criteria** |  
|-------------------------|--------------------------|  
| Applies to the **entire Increment**. | Applies to **individual PBIs**. |  
| Focuses on **technical and quality standards**. | Focuses on **functional requirements**. |  
| Consistent across all PBIs in a Sprint. | Unique to each PBI. |  

---

### **Why the DoD Isn’t Defined per PBI**
   - **Consistency**: The DoD ensures all work in the Increment meets the same quality bar, regardless of the PBI.  
   - **Predictability**: Teams can reliably forecast how much they can deliver in a Sprint because the DoD’s scope is fixed.  
   - **Avoids Duplication**: If the DoD were defined per PBI, teams would waste time redefining standards for every item.  

---

### **How They Work Together**  
For a PBI to be included in the Increment:  
1. It must meet its **acceptance criteria** (functional completeness).  
2. The work to deliver it must satisfy the **DoD** (technical completeness).  

**Example**:  
- A user story (PBI) for a login feature might have acceptance criteria like:  
  - "User can log in with email and password."  
  - "Error message displays for invalid credentials."  
- To meet the DoD, the team must also:  
  - Write unit tests for the login feature.  
  - Ensure accessibility standards are met.  
  - Deploy the code to a staging environment.  

---

### **Risks of Confusing DoD with PBI Acceptance Criteria**
   - **Inconsistent Quality**: PBIs might meet functional needs but lack technical rigor (e.g., untested code).  
   - **Scope Creep**: Teams might treat the DoD as optional for individual PBIs, leading to incomplete work.  
   - **Misalignment**: Stakeholders might assume a "done" PBI is shippable, only to discover technical debt later.  

---

### **Key Takeaway**  
- **DoD**: A **non-negotiable checklist** for the **Increment** (applies to all PBIs).  
- **Acceptance Criteria**: A **custom checklist** for **individual PBIs** (defines functionality).  
- Both are essential: The DoD ensures technical quality, while acceptance criteria ensure the right functionality is built.  

By separating these concepts, Scrum teams maintain both **functional correctness** and **technical integrity** in their deliverables.


**Functional requirements** in software engineering define **what a system should do**—the specific behaviors, features, and functions it must perform to meet user and stakeholder needs. They describe the system’s intended capabilities, inputs, outputs, and interactions in detail. Here’s a breakdown:

---

## functional requirements
<thoughts>
Okay, the user is asking about functional requirements in software engineering. Let me start by recalling what functional requirements are. They define what the software should do, right? So, the user probably wants a clear explanation of what these requirements are, maybe examples, and how they differ from non-functional ones.

First, I should define functional requirements. They are specific behaviors or functions that the system must perform. For example, in a login system, a functional requirement might be allowing users to reset their password. I need to make sure the explanation is straightforward.

Then, I should provide examples. Maybe break them down into categories like user requirements, system functions, business rules, and data handling. That structure will help the user understand different contexts where functional requirements apply.

Next, I should explain how functional requirements are different from non-functional. Non-functional are about how the system works, like performance or security, while functional are about what the system does. Including a comparison table would make this distinction clear.

The user might also be interested in why functional requirements are important. So, I should mention aspects like guiding development, ensuring alignment with user needs, and serving as a basis for testing. It's crucial to highlight their role in the development lifecycle.

I should also think about how these requirements are documented. Use cases, user stories, and functional specifications are common methods. Providing examples of each, like a user story format, can be helpful.

Maybe include best practices too. Things like being specific, traceable, and avoiding technical jargon. This helps the user understand how to write effective functional requirements.

Wait, the user is asking for "functional requirements for software engineering." Are they a student, a new developer, or someone involved in project management? The answer should be accessible to someone without deep technical knowledge but thorough enough to be useful.

Also, check if there are any common mistakes people make when defining functional requirements, like being too vague. Including pitfalls to avoid would add value.

Let me organize this: definition, examples, differences from non-functional, importance, documentation methods, best practices, and common pitfalls. That structure should cover the topic comprehensively.

I need to make sure the language is clear and avoid unnecessary jargon. Use bullet points and tables where possible for readability. Including real-world examples, like an e-commerce system, can make the concepts more concrete.

Double-check that all information is accurate. For example, functional requirements vs. non-functional: functional is about what, non-functional about how well. Examples of each are correct. The documentation methods like use cases and user stories are appropriate.

Also, mention that functional requirements are part of the Software Requirements Specification (SRS) document. That ties it into standard practices.

Finally, wrap it up with a key takeaway to reinforce the main points. Make sure the answer is thorough but not overwhelming, balancing depth with clarity.
</thoughts>

### **1. What Are Functional Requirements?**
Functional requirements specify **actions the system must perform** to deliver value to users. They answer questions like:
- What tasks should the system accomplish?
- How should the system respond to specific inputs?
- What outputs or results are expected?

**Example**:  
For an e-commerce app, functional requirements might include:  
- Users can search for products by name or category.  
- Users can add items to a shopping cart.  
- The system calculates taxes and shipping costs during checkout.  

---

### **2. Key Components of Functional Requirements**
Functional requirements typically describe:  
- **User interactions** (e.g., login, data entry, reporting).  
- **Business rules** (e.g., discounts apply only to orders over $50).  
- **Data handling** (e.g., how data is stored, retrieved, or validated).  
- **System integrations** (e.g., connecting to payment gateways or APIs).  
- **Workflows** (e.g., order approval steps for a manager).  

---

### **3. Examples of Functional Requirements**
| **Category**          | **Example**                                                                 |
|------------------------|-----------------------------------------------------------------------------|
| **User Actions**       | "The user can upload a profile photo in JPEG or PNG format."                |
| **Data Validation**    | "The system must validate email addresses using regex before submission."   |
| **Business Logic**     | "A 10% discount is applied if the user spends over $100."                   |
| **Reporting**          | "Generate a monthly sales report in PDF format."                            |
| **Integration**        | "The app must integrate with PayPal for payment processing."                |

---

### **4. Functional vs. Non-Functional Requirements**
| **Functional Requirements**                     | **Non-Functional Requirements**                      |  
|--------------------------------------------------|------------------------------------------------------|  
| Define **what** the system does.                 | Define **how well** the system performs.             |  
| Focus on **features and behaviors**.             | Focus on **quality attributes** (e.g., speed, security). |  
| Example: "Users can reset passwords via email."  | Example: "The system must load pages in under 2 seconds." |  

---

### **5. Why Are Functional Requirements Important?**
- **Guide Development**: Provide clear instructions for developers and testers.  
- **Align Stakeholders**: Ensure everyone agrees on what the system should do.  
- **Basis for Testing**: Enable test cases to verify if the system meets expectations.  
- **Reduce Ambiguity**: Prevent scope creep or misunderstandings during development.  

---

### **6. How Are Functional Requirements Documented?**
They are often captured in:  
- **Use Cases**: Scenarios describing user-system interactions (e.g., "Place Order").  
- **User Stories**: Agile-style statements like:  
  *"As a customer, I want to filter products by price so I can find affordable items."*  
- **Software Requirements Specification (SRS)**: A formal document detailing all requirements.  
- **Wireframes/Mockups**: Visual representations of features (e.g., UI layouts).  

---

### **7. Best Practices for Writing Functional Requirements**
- **Be Specific**: Avoid vague terms like "user-friendly" (e.g., "The search bar must autocomplete after 3 keystrokes").  
- **Use Active Voice**: "The system **shall** validate user input" instead of passive phrasing.  
- **Prioritize**: Rank requirements (e.g., "must-have" vs. "nice-to-have").  
- **Ensure Traceability**: Link requirements to design, code, and test cases.  

---

### **8. Common Pitfalls to Avoid**  
- **Overlooking Edge Cases**: Failing to define how the system handles errors or unusual inputs.  
- **Mixing with Non-Functional**: Confusing performance goals (e.g., "fast") with functionality.  
- **Ambiguity**: Using terms like "support" without clarity (e.g., "support mobile devices" vs. "optimized for iOS and Android").  

---

### **Key Takeaway**  
Functional requirements are the **backbone of software development**. They translate user needs into actionable specifications, ensuring the final product delivers the right features and behaviors. Without them, teams risk building a system that fails to meet expectations or solve real-world problems.
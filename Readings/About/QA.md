Vinny Monteiro, a business transformation manager at JP Morgan Chase
on delivering exceptional products

### The Quality Equation

a powerful concept that helps us balance the value we deliver to users with the cost of development

The key lies in maximizing the value for the cost of the product or feature. 
If you focus on delivering features and functionalities that truly matter to users, you can maximize value while keeping development costs under control. 

The quality equation highlights the cost effectiveness of proactive quality measures. 

The quality equation reminds us that there is a tipping point. While striving for excellence is important, obsessing over every tiny detail can be counterproductive. 

Imagine going on a long hike. You need to balance between traveling light versus bringing everything you need because you would get bogged down by carrying an excessive amount of weight. 

You should focus on features and functionalities that deliver real value to users, but avoid getting hung up on excessive perfectionism or features that add little value. 


# ask deepseek about judgemental call is DoD
should "Definition of Done" contain some subjective judgmental calls

Thought for 18 seconds

Okay, so the user is asking whether the "Definition of Done" (DoD) should include some subjective judgment calls. Let me start by recalling what the DoD is. From what I know, in Agile methodologies like Scrum, the DoD is a set of criteria that a product increment must meet to be considered complete. It's supposed to be a clear and shared understanding among the team.

Now, the question is about whether there's room for subjectivity in that. My first thought is that ideally, the DoD should be as objective as possible to avoid ambiguity. If it's subjective, different team members might interpret it differently, leading to inconsistencies. For example, if a DoD item is "Code is well-tested," that's pretty vague. What does "well-tested" mean? One developer might think unit tests are enough, another might think integration tests are necessary. That could cause problems in quality or delays.

But maybe there are areas where some subjectivity is unavoidable. Let's say a team includes a requirement like "User interface is intuitive." Intuitiveness is subjective by nature. It's based on user perception, which can vary. So in such cases, how does the team handle that? They might need to rely on user testing or expert reviews, which introduces some judgment but based on feedback rather than personal opinion.

Another angle is the maturity of the team. Newer teams might have a less refined DoD that includes more subjective measures because they're still learning. Over time, as they gain experience, they can replace those subjective items with more objective ones. For instance, instead of saying "Code is clean," they might adopt specific coding standards or use static analysis tools to measure code quality.

Also, different contexts might require different approaches. In creative fields like design or content creation, some subjectivity is inherent. A DoD for a design task might include "Approved by the client," which is a subjective judgment but necessary. However, even there, teams can aim to make the approval criteria as clear as possible to reduce ambiguity.

I should also consider the balance between flexibility and rigor. Too much subjectivity might lead to scope creep or incomplete work, but too much rigidity could stifle creativity or adaptability. The key is finding a middle ground where the DoD is mostly objective but allows for expert judgment in areas where it's needed.

Examples might help. If the DoD includes "All tests pass," that's objective. If it includes "Code reviewed by two peers," that's also objective. But if it includes "User story meets acceptance criteria," and the acceptance criteria are vague, then it's subjective. So the solution is to make sure acceptance criteria are well-defined.

In conclusion, while the DoD should primarily be objective to ensure consistency and clarity, there might be scenarios where some subjectivity is necessary. However, teams should strive to minimize it by defining clear, measurable criteria wherever possible and using agreed-upon processes (like user testing or expert review) to handle areas where subjectivity can't be eliminated entirely.

The **Definition of Done (DoD)** should ideally minimize subjectivity to ensure consistency, transparency, and shared understanding within a team. However, in practice, some judgment may be unavoidable depending on context. Here's a breakdown:

### **1. Objective Criteria Are Preferred**

- **Clarity and Consistency**: Objective criteria (e.g., "All unit tests pass," "Code reviewed by two peers") reduce ambiguity and ensure everyone agrees on what "done" means. This prevents disagreements and uneven quality.
    
- **Measurability**: Metrics like test coverage percentages, compliance with coding standards, or automated checks leave little room for interpretation.
    

### **2. When Subjectivity _Might_ Creep In**

- **Design/Creative Work**: Criteria like "UI aligns with brand guidelines" or "user-friendly" may require expert judgment. Teams can mitigate this by referencing style guides, user testing, or stakeholder sign-offs.
    
- **Complex or Novel Problems**: Early-stage projects might lack clear benchmarks, requiring experienced team members to assess readiness (e.g., "solution is scalable enough for MVP").
    
- **Cultural Norms**: Teams may implicitly rely on shared standards (e.g., "clean code"), but these should evolve into explicit, measurable rules over time.
    

### **3. Mitigating Subjectivity**

- **Refine the DoD Iteratively**: Replace vague terms (e.g., "well-tested") with specifics (e.g., "80% test coverage, integration tests included").
    
- **Use Guardrails for Judgment**: If subjective criteria are unavoidable, define _who_ decides (e.g., "approved by UX lead") or _how_ (e.g., "validated via user testing with 5+ participants").
    
- **Balance Flexibility and Rigor**: In creative fields, pair subjective goals (e.g., "client approves") with objective checks (e.g., "meets accessibility standards").
    

### **4. Risks of Excessive Subjectivity**

- Inconsistent quality, scope creep, or delays if team members interpret criteria differently.
    
- Erosion of trust if stakeholders perceive the DoD as arbitrary.
    

### **Conclusion**

Aim for **objectivity first**, but acknowledge that **some judgment may be necessary** in nuanced scenarios. Teams should document and refine the DoD collaboratively to minimize ambiguity while allowing flexibility where expertise is required. Over time, replace subjective elements with measurable standards as the team matures.